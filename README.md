# deep-speaker
## Speech identification with deep speaker 

Dataset: not public

Reference Paper: https://arxiv.org/pdf/1705.02304.pdf

Reference Code: https://github.com/philipperemy/deep-speaker

## Introduction
The embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering. We experiment with ResCNN architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity.

## About my code
This part i will use vietnamese, if i have time in the future, i will translate it to english.

#### ```pre_process.py```

#### Mục đích: Xử lý file wav. Trích chọn đặc trưng file raw audio, lưu dưới dạng file numpy array.

#### Chi tiết: 

  ```def VAD(audio)``` : Hàm tiền xử lí audio, lọc âm trắng theo từng chunk_size (s).
  
  ```def read_audio(filename, sample_rate=SAMPLE_RATE)``` : Hàm đọc file raw audio, dùng thư viện librosa để load
  âm thanh với tần số lấy mẫu là SAMPLE_RATE = 16000. Sample_rate là gì? Vậy tại sao không lấy mẫu tại 44000? 
  Tham khảo [tại đây.](https://librosa.github.io/blog/2019/07/17/resample-on-load/) 
  Ngoài ra, mình chỉ lấy 3s/audio, đây là 1 điểm hạn chế mà mình sẽ sửa chữa sớm, vì điều này làm mất data với những file audio lớn.
  
  ```def normalize_frames(m,epsilon=1e-12)``` : Hàm chuẩn hóa đầu vào mạng deep learning, nó cho phép sử dụng tốt hơn 
  learning rate, khởi tạo hiệu quả hơn. Chi tiết tại đây: [Batch normalize](https://arxiv.org/pdf/1502.03167.pdf)
  
  ```def extract_features(signal, target_sample_rate=SAMPLE_RATE)``` : 
  Bước đầu tiên trong bất kỳ hệ thống nhận dạng giọng nói tự động nào là trích xuất các tính năng, 
  tức là xác định các thành phần của tín hiệu âm thanh tốt để xác định nội dung ngôn ngữ và 
  loại bỏ tất cả các nội dung khác mang thông tin như nhiễu nền, cảm xúc, v.v. Để hiểu rõ hơn về mel filterbank, 
  tham khảo [MFCC](http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)
  
  Sau khi qua hàm này, raw audio trở thành 1 np.array với shape (num_frames, nfilt(trong fbank), 1)
  
#### ```models.py```

#### Chi tiết:

Model được implement lại giống y hệt trong paper, sử dụng 4 res_block với số lượng filters lần lượt là 64, 128, 256, 512 
để trích chọn đặc trưng của tín hiệu âm thanh.


#### ```random_batch.py```

#### Mục đích: Tạo batch random để làm input cho pretrain model với loss softmax.

#### Chi tiết:
